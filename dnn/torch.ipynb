{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,reshape\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import mnist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = reshape(x,(-1,784))\n",
    "        h = F.sigmoid(self.fc1(x))\n",
    "        h = F.sigmoid(self.fc2(h))\n",
    "        h = F.softmax(self.fc3(h))\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Net(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "[transforms.ToTensor()]\n",
    "    )\n",
    "trainset = torchvision.datasets.MNIST(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                      transform=transform\n",
    "                                        )\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       "  tensor([7, 2, 4, 6, 0, 4, 2, 0, 6, 7, 9, 2, 0, 0, 9, 5, 5, 7, 8, 8, 7, 5, 3, 8,\n",
       "          3, 9, 5, 7, 6, 6, 1, 5, 4, 6, 1, 8, 6, 3, 9, 6, 3, 2, 0, 0, 2, 5, 9, 8,\n",
       "          6, 7, 7, 7, 9, 3, 6, 7, 7, 3, 2, 7, 8, 8, 9, 0, 4, 2, 9, 2, 9, 7, 7, 3,\n",
       "          2, 4, 6, 6, 0, 7, 3, 8, 0, 2, 1, 8, 6, 9, 5, 6, 3, 8, 7, 9, 7, 8, 4, 8,\n",
       "          7, 9, 6, 3])])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(trainloader,0).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = torchvision.datasets.MNIST(root='./data', \n",
    "                                        train=False, \n",
    "                                        download=True, \n",
    "                                        )\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsimluken/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.303617000579834\n",
      "loss:2.3033902645111084\n",
      "loss:2.300980806350708\n",
      "loss:2.303358793258667\n",
      "loss:2.3046979904174805\n",
      "loss:2.304670572280884\n",
      "loss:2.304590940475464\n",
      "loss:2.300488233566284\n",
      "loss:2.3038196563720703\n",
      "loss:2.3022301197052\n",
      "loss:2.3051018714904785\n",
      "loss:2.3061811923980713\n",
      "loss:2.3044183254241943\n",
      "loss:2.3025574684143066\n",
      "loss:2.302853584289551\n",
      "loss:2.3020317554473877\n",
      "loss:2.304358959197998\n",
      "loss:2.305622100830078\n",
      "loss:2.3037962913513184\n",
      "loss:2.302271604537964\n",
      "loss:2.3012051582336426\n",
      "loss:2.3029630184173584\n",
      "loss:2.3052632808685303\n",
      "loss:2.304157257080078\n",
      "loss:2.301630735397339\n",
      "loss:2.3017711639404297\n",
      "loss:2.3048899173736572\n",
      "loss:2.3024325370788574\n",
      "loss:2.3051400184631348\n",
      "loss:2.305893659591675\n",
      "loss:2.30501651763916\n",
      "loss:2.300947666168213\n",
      "loss:2.303271532058716\n",
      "loss:2.303037643432617\n",
      "loss:2.304123640060425\n",
      "loss:2.302429676055908\n",
      "loss:2.303151845932007\n",
      "loss:2.3041722774505615\n",
      "loss:2.303926706314087\n",
      "loss:2.3034141063690186\n",
      "loss:2.3053054809570312\n",
      "loss:2.303588628768921\n",
      "loss:2.300611734390259\n",
      "loss:2.3056588172912598\n",
      "loss:2.301800012588501\n",
      "loss:2.300896167755127\n",
      "loss:2.304715633392334\n",
      "loss:2.303286552429199\n",
      "loss:2.303886890411377\n",
      "loss:2.3014988899230957\n",
      "loss:2.303743362426758\n",
      "loss:2.3014588356018066\n",
      "loss:2.3042681217193604\n",
      "loss:2.3043956756591797\n",
      "loss:2.3011345863342285\n",
      "loss:2.302635908126831\n",
      "loss:2.302398681640625\n",
      "loss:2.304760694503784\n",
      "loss:2.3027420043945312\n",
      "loss:2.3055973052978516\n",
      "loss:2.306230068206787\n",
      "loss:2.3058855533599854\n",
      "loss:2.300055503845215\n",
      "loss:2.3015689849853516\n",
      "loss:2.300812244415283\n",
      "loss:2.3026363849639893\n",
      "loss:2.303593635559082\n",
      "loss:2.302332878112793\n",
      "loss:2.303765296936035\n",
      "loss:2.2996575832366943\n",
      "loss:2.3023433685302734\n",
      "loss:2.303464889526367\n",
      "loss:2.3055779933929443\n",
      "loss:2.302466869354248\n",
      "loss:2.301961660385132\n",
      "loss:2.3016602993011475\n",
      "loss:2.303088903427124\n",
      "loss:2.3018410205841064\n",
      "loss:2.305382013320923\n",
      "loss:2.3040876388549805\n",
      "loss:2.300525188446045\n",
      "loss:2.3024680614471436\n",
      "loss:2.3037354946136475\n",
      "loss:2.303095579147339\n",
      "loss:2.300581455230713\n",
      "loss:2.303497076034546\n",
      "loss:2.30195951461792\n",
      "loss:2.305877447128296\n",
      "loss:2.3021187782287598\n",
      "loss:2.30120849609375\n",
      "loss:2.3019039630889893\n",
      "loss:2.302954912185669\n",
      "loss:2.3015787601470947\n",
      "loss:2.301300287246704\n",
      "loss:2.307149648666382\n",
      "loss:2.3055121898651123\n",
      "loss:2.303163528442383\n",
      "loss:2.303375482559204\n",
      "loss:2.304136276245117\n",
      "loss:2.3035664558410645\n",
      "loss:2.3038909435272217\n",
      "loss:2.3023972511291504\n",
      "loss:2.3039090633392334\n",
      "loss:2.3021135330200195\n",
      "loss:2.306727886199951\n",
      "loss:2.300569772720337\n",
      "loss:2.303694725036621\n",
      "loss:2.3029205799102783\n",
      "loss:2.3033716678619385\n",
      "loss:2.3022329807281494\n",
      "loss:2.3036389350891113\n",
      "loss:2.3039705753326416\n",
      "loss:2.3032493591308594\n",
      "loss:2.3014538288116455\n",
      "loss:2.301325798034668\n",
      "loss:2.3015942573547363\n",
      "loss:2.302680015563965\n",
      "loss:2.3026132583618164\n",
      "loss:2.3038105964660645\n",
      "loss:2.3039584159851074\n",
      "loss:2.3038902282714844\n",
      "loss:2.3033766746520996\n",
      "loss:2.299537420272827\n",
      "loss:2.3025283813476562\n",
      "loss:2.3028314113616943\n",
      "loss:2.3038156032562256\n",
      "loss:2.304229736328125\n",
      "loss:2.305554151535034\n",
      "loss:2.3028604984283447\n",
      "loss:2.303659677505493\n",
      "loss:2.3018414974212646\n",
      "loss:2.302926540374756\n",
      "loss:2.3027727603912354\n",
      "loss:2.3060476779937744\n",
      "loss:2.3025693893432617\n",
      "loss:2.30203914642334\n",
      "loss:2.304905414581299\n",
      "loss:2.30161714553833\n",
      "loss:2.3032164573669434\n",
      "loss:2.302371025085449\n",
      "loss:2.301403045654297\n",
      "loss:2.303407907485962\n",
      "loss:2.3017988204956055\n",
      "loss:2.3049509525299072\n",
      "loss:2.3040246963500977\n",
      "loss:2.30466365814209\n",
      "loss:2.302617311477661\n",
      "loss:2.3019490242004395\n",
      "loss:2.3038315773010254\n",
      "loss:2.3023037910461426\n",
      "loss:2.3035573959350586\n",
      "loss:2.303443670272827\n",
      "loss:2.3026022911071777\n",
      "loss:2.3030638694763184\n",
      "loss:2.3050966262817383\n",
      "loss:2.303474187850952\n",
      "loss:2.3000543117523193\n",
      "loss:2.3020823001861572\n",
      "loss:2.30269718170166\n",
      "loss:2.302940607070923\n",
      "loss:2.3029580116271973\n",
      "loss:2.3035035133361816\n",
      "loss:2.303497076034546\n",
      "loss:2.303112268447876\n",
      "loss:2.3035728931427\n",
      "loss:2.2997825145721436\n",
      "loss:2.3014564514160156\n",
      "loss:2.304929494857788\n",
      "loss:2.301490306854248\n",
      "loss:2.3036646842956543\n",
      "loss:2.304537534713745\n",
      "loss:2.303366184234619\n",
      "loss:2.3022162914276123\n",
      "loss:2.3033227920532227\n",
      "loss:2.3036956787109375\n",
      "loss:2.304271936416626\n",
      "loss:2.3047029972076416\n",
      "loss:2.303067684173584\n",
      "loss:2.3034520149230957\n",
      "loss:2.303575277328491\n",
      "loss:2.3038620948791504\n",
      "loss:2.3041608333587646\n",
      "loss:2.301889181137085\n",
      "loss:2.3028976917266846\n",
      "loss:2.3034441471099854\n",
      "loss:2.304013967514038\n",
      "loss:2.3035547733306885\n",
      "loss:2.299825429916382\n",
      "loss:2.3018839359283447\n",
      "loss:2.3017265796661377\n",
      "loss:2.3056230545043945\n",
      "loss:2.3046717643737793\n",
      "loss:2.3055243492126465\n",
      "loss:2.3027219772338867\n",
      "loss:2.30340576171875\n",
      "loss:2.3050127029418945\n",
      "loss:2.3061282634735107\n",
      "loss:2.3024439811706543\n",
      "loss:2.304060697555542\n",
      "loss:2.302356004714966\n",
      "loss:2.3003506660461426\n",
      "loss:2.3030529022216797\n",
      "loss:2.300795555114746\n",
      "loss:2.3061766624450684\n",
      "loss:2.302783966064453\n",
      "loss:2.3051350116729736\n",
      "loss:2.3048901557922363\n",
      "loss:2.301670551300049\n",
      "loss:2.3027052879333496\n",
      "loss:2.3033406734466553\n",
      "loss:2.3033432960510254\n",
      "loss:2.304765462875366\n",
      "loss:2.3039238452911377\n",
      "loss:2.3033621311187744\n",
      "loss:2.302739381790161\n",
      "loss:2.3011045455932617\n",
      "loss:2.3003742694854736\n",
      "loss:2.301201820373535\n",
      "loss:2.3040621280670166\n",
      "loss:2.3019235134124756\n",
      "loss:2.3024449348449707\n",
      "loss:2.303598642349243\n",
      "loss:2.3056180477142334\n",
      "loss:2.301309823989868\n",
      "loss:2.303378105163574\n",
      "loss:2.3023743629455566\n",
      "loss:2.306093692779541\n",
      "loss:2.303098678588867\n",
      "loss:2.303626775741577\n",
      "loss:2.303705930709839\n",
      "loss:2.303297996520996\n",
      "loss:2.302471876144409\n",
      "loss:2.3017542362213135\n",
      "loss:2.3047683238983154\n",
      "loss:2.302978754043579\n",
      "loss:2.303675889968872\n",
      "loss:2.305518865585327\n",
      "loss:2.3021481037139893\n",
      "loss:2.3050127029418945\n",
      "loss:2.3017425537109375\n",
      "loss:2.3027498722076416\n",
      "loss:2.3029115200042725\n",
      "loss:2.3015329837799072\n",
      "loss:2.304361581802368\n",
      "loss:2.303835391998291\n",
      "loss:2.303333044052124\n",
      "loss:2.303060531616211\n",
      "loss:2.30129337310791\n",
      "loss:2.305528402328491\n",
      "loss:2.3051438331604004\n",
      "loss:2.2997260093688965\n",
      "loss:2.301980495452881\n",
      "loss:2.3032007217407227\n",
      "loss:2.301668643951416\n",
      "loss:2.3048934936523438\n",
      "loss:2.303098201751709\n",
      "loss:2.302713394165039\n",
      "loss:2.3038253784179688\n",
      "loss:2.3022546768188477\n",
      "loss:2.302345037460327\n",
      "loss:2.3021857738494873\n",
      "loss:2.3033885955810547\n",
      "loss:2.3058648109436035\n",
      "loss:2.3033225536346436\n",
      "loss:2.301882266998291\n",
      "loss:2.3049497604370117\n",
      "loss:2.3031868934631348\n",
      "loss:2.3031415939331055\n",
      "loss:2.3030309677124023\n",
      "loss:2.3026862144470215\n",
      "loss:2.3038694858551025\n",
      "loss:2.302855968475342\n",
      "loss:2.3020191192626953\n",
      "loss:2.3042118549346924\n",
      "loss:2.3025145530700684\n",
      "loss:2.3024516105651855\n",
      "loss:2.3015034198760986\n",
      "loss:2.3025882244110107\n",
      "loss:2.3065168857574463\n",
      "loss:2.3048951625823975\n",
      "loss:2.301097869873047\n",
      "loss:2.300673007965088\n",
      "loss:2.300870656967163\n",
      "loss:2.304316282272339\n",
      "loss:2.3018689155578613\n",
      "loss:2.30444598197937\n",
      "loss:2.3019473552703857\n",
      "loss:2.302579402923584\n",
      "loss:2.3029444217681885\n",
      "loss:2.3042383193969727\n",
      "loss:2.3025074005126953\n",
      "loss:2.302258253097534\n",
      "loss:2.302098274230957\n",
      "loss:2.301971912384033\n",
      "loss:2.303701162338257\n",
      "loss:2.3029236793518066\n",
      "loss:2.3051304817199707\n",
      "loss:2.3013715744018555\n",
      "loss:2.302417039871216\n",
      "loss:2.3042733669281006\n",
      "loss:2.30119252204895\n",
      "loss:2.303981304168701\n",
      "loss:2.302560567855835\n",
      "loss:2.3040285110473633\n",
      "loss:2.303621768951416\n",
      "loss:2.3000056743621826\n",
      "loss:2.3048739433288574\n",
      "loss:2.3035521507263184\n",
      "loss:2.3036680221557617\n",
      "loss:2.305276870727539\n",
      "loss:2.3018336296081543\n",
      "loss:2.3060920238494873\n",
      "loss:2.303189754486084\n",
      "loss:2.302753210067749\n",
      "loss:2.303720712661743\n",
      "loss:2.3019773960113525\n",
      "loss:2.3025879859924316\n",
      "loss:2.3032941818237305\n",
      "loss:2.3041837215423584\n",
      "loss:2.3027303218841553\n",
      "loss:2.3061985969543457\n",
      "loss:2.3011558055877686\n",
      "loss:2.304119825363159\n",
      "loss:2.3043904304504395\n",
      "loss:2.3046820163726807\n",
      "loss:2.3034071922302246\n",
      "loss:2.3041701316833496\n",
      "loss:2.304123878479004\n",
      "loss:2.303079128265381\n",
      "loss:2.3045480251312256\n",
      "loss:2.3035781383514404\n",
      "loss:2.3012657165527344\n",
      "loss:2.3032822608947754\n",
      "loss:2.3037707805633545\n",
      "loss:2.302741527557373\n",
      "loss:2.3059351444244385\n",
      "loss:2.303287982940674\n",
      "loss:2.3003227710723877\n",
      "loss:2.302931308746338\n",
      "loss:2.30353045463562\n",
      "loss:2.3030264377593994\n",
      "loss:2.3047232627868652\n",
      "loss:2.302990198135376\n",
      "loss:2.301182508468628\n",
      "loss:2.3049721717834473\n",
      "loss:2.304791212081909\n",
      "loss:2.3033478260040283\n",
      "loss:2.301680326461792\n",
      "loss:2.3009259700775146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.30367112159729\n",
      "loss:2.302899122238159\n",
      "loss:2.3034732341766357\n",
      "loss:2.3041656017303467\n",
      "loss:2.302762508392334\n",
      "loss:2.3047940731048584\n",
      "loss:2.302969455718994\n",
      "loss:2.303703784942627\n",
      "loss:2.3009848594665527\n",
      "loss:2.303772449493408\n",
      "loss:2.3021578788757324\n",
      "loss:2.3025870323181152\n",
      "loss:2.3030846118927\n",
      "loss:2.3015735149383545\n",
      "loss:2.301715612411499\n",
      "loss:2.3051977157592773\n",
      "loss:2.302591323852539\n",
      "loss:2.3022429943084717\n",
      "loss:2.305255889892578\n",
      "loss:2.3021914958953857\n",
      "loss:2.3058278560638428\n",
      "loss:2.303717613220215\n",
      "loss:2.3050570487976074\n",
      "loss:2.3020195960998535\n",
      "loss:2.3032705783843994\n",
      "loss:2.3050520420074463\n",
      "loss:2.303910970687866\n",
      "loss:2.301849126815796\n",
      "loss:2.3019585609436035\n",
      "loss:2.3032662868499756\n",
      "loss:2.3020150661468506\n",
      "loss:2.302302837371826\n",
      "loss:2.3029592037200928\n",
      "loss:2.305222272872925\n",
      "loss:2.301119565963745\n",
      "loss:2.3028581142425537\n",
      "loss:2.3044023513793945\n",
      "loss:2.303152561187744\n",
      "loss:2.3004534244537354\n",
      "loss:2.3041434288024902\n",
      "loss:2.3038322925567627\n",
      "loss:2.3032729625701904\n",
      "loss:2.302064895629883\n",
      "loss:2.3047966957092285\n",
      "loss:2.305004596710205\n",
      "loss:2.3054039478302\n",
      "loss:2.30415940284729\n",
      "loss:2.3035123348236084\n",
      "loss:2.30151629447937\n",
      "loss:2.303183078765869\n",
      "loss:2.3007397651672363\n",
      "loss:2.3039090633392334\n",
      "loss:2.3038814067840576\n",
      "loss:2.3016316890716553\n",
      "loss:2.3035662174224854\n",
      "loss:2.303673028945923\n",
      "loss:2.30100154876709\n",
      "loss:2.3058512210845947\n",
      "loss:2.304042100906372\n",
      "loss:2.3053481578826904\n",
      "loss:2.303589105606079\n",
      "loss:2.301784038543701\n",
      "loss:2.3019399642944336\n",
      "loss:2.304514169692993\n",
      "loss:2.3001887798309326\n",
      "loss:2.3040060997009277\n",
      "loss:2.303659677505493\n",
      "loss:2.301314353942871\n",
      "loss:2.302050828933716\n",
      "loss:2.3055179119110107\n",
      "loss:2.302517890930176\n",
      "loss:2.303312063217163\n",
      "loss:2.3047516345977783\n",
      "loss:2.3024752140045166\n",
      "loss:2.302356004714966\n",
      "loss:2.3040871620178223\n",
      "loss:2.306774854660034\n",
      "loss:2.302161931991577\n",
      "loss:2.3024535179138184\n",
      "loss:2.3017265796661377\n",
      "loss:2.3036792278289795\n",
      "loss:2.302250862121582\n",
      "loss:2.303563117980957\n",
      "loss:2.3022830486297607\n",
      "loss:2.3023080825805664\n",
      "loss:2.3035523891448975\n",
      "loss:2.3043456077575684\n",
      "loss:2.3039746284484863\n",
      "loss:2.3025548458099365\n",
      "loss:2.3039348125457764\n",
      "loss:2.3026769161224365\n",
      "loss:2.3043830394744873\n",
      "loss:2.3052585124969482\n",
      "loss:2.299201726913452\n",
      "loss:2.3051791191101074\n",
      "loss:2.304960250854492\n",
      "loss:2.303075075149536\n",
      "loss:2.3017706871032715\n",
      "loss:2.3016932010650635\n",
      "loss:2.306212902069092\n",
      "loss:2.303205966949463\n",
      "loss:2.302128553390503\n",
      "loss:2.3020401000976562\n",
      "loss:2.301515817642212\n",
      "loss:2.3039870262145996\n",
      "loss:2.3028390407562256\n",
      "loss:2.3035740852355957\n",
      "loss:2.302151679992676\n",
      "loss:2.3049395084381104\n",
      "loss:2.3039956092834473\n",
      "loss:2.3041534423828125\n",
      "loss:2.304319143295288\n",
      "loss:2.3041794300079346\n",
      "loss:2.3028903007507324\n",
      "loss:2.298994541168213\n",
      "loss:2.3033077716827393\n",
      "loss:2.301436185836792\n",
      "loss:2.304576873779297\n",
      "loss:2.3042449951171875\n",
      "loss:2.3032379150390625\n",
      "loss:2.3016233444213867\n",
      "loss:2.3014981746673584\n",
      "loss:2.29978609085083\n",
      "loss:2.300863027572632\n",
      "loss:2.303335428237915\n",
      "loss:2.3037781715393066\n",
      "loss:2.301264762878418\n",
      "loss:2.300694465637207\n",
      "loss:2.305433511734009\n",
      "loss:2.3049960136413574\n",
      "loss:2.3022279739379883\n",
      "loss:2.30509877204895\n",
      "loss:2.3073031902313232\n",
      "loss:2.3019654750823975\n",
      "loss:2.305389404296875\n",
      "loss:2.3031656742095947\n",
      "loss:2.3030786514282227\n",
      "loss:2.3028767108917236\n",
      "loss:2.3033688068389893\n",
      "loss:2.303276777267456\n",
      "loss:2.305467367172241\n",
      "loss:2.3036210536956787\n",
      "loss:2.303698778152466\n",
      "loss:2.3032045364379883\n",
      "loss:2.3022165298461914\n",
      "loss:2.3038625717163086\n",
      "loss:2.3030455112457275\n",
      "loss:2.3014485836029053\n",
      "loss:2.300987482070923\n",
      "loss:2.305994987487793\n",
      "loss:2.302306890487671\n",
      "loss:2.3016786575317383\n",
      "loss:2.301708698272705\n",
      "loss:2.3035104274749756\n",
      "loss:2.304381847381592\n",
      "loss:2.3027751445770264\n",
      "loss:2.30472731590271\n",
      "loss:2.303759813308716\n",
      "loss:2.303741216659546\n",
      "loss:2.304184675216675\n",
      "loss:2.3019731044769287\n",
      "loss:2.3021976947784424\n",
      "loss:2.3019137382507324\n",
      "loss:2.306734561920166\n",
      "loss:2.303513288497925\n",
      "loss:2.304391384124756\n",
      "loss:2.3020219802856445\n",
      "loss:2.3028712272644043\n",
      "loss:2.304539203643799\n",
      "loss:2.303802490234375\n",
      "loss:2.3003101348876953\n",
      "loss:2.304736375808716\n",
      "loss:2.302913188934326\n",
      "loss:2.305783748626709\n",
      "loss:2.3050801753997803\n",
      "loss:2.3042514324188232\n",
      "loss:2.3035943508148193\n",
      "loss:2.305330991744995\n",
      "loss:2.3042409420013428\n",
      "loss:2.303788900375366\n",
      "loss:2.3020055294036865\n",
      "loss:2.302802085876465\n",
      "loss:2.3004848957061768\n",
      "loss:2.303488254547119\n",
      "loss:2.302438497543335\n",
      "loss:2.3032162189483643\n",
      "loss:2.301980495452881\n",
      "loss:2.304504632949829\n",
      "loss:2.30334734916687\n",
      "loss:2.3036677837371826\n",
      "loss:2.302537441253662\n",
      "loss:2.303948402404785\n",
      "loss:2.302701950073242\n",
      "loss:2.302356243133545\n",
      "loss:2.3015804290771484\n",
      "loss:2.2996106147766113\n",
      "loss:2.303497076034546\n",
      "loss:2.304786205291748\n",
      "loss:2.301449775695801\n",
      "loss:2.3017141819000244\n",
      "loss:2.304227828979492\n",
      "loss:2.3037354946136475\n",
      "loss:2.3034446239471436\n",
      "loss:2.304210662841797\n",
      "loss:2.3037378787994385\n",
      "loss:2.302304744720459\n",
      "loss:2.3046021461486816\n",
      "loss:2.304241418838501\n",
      "loss:2.3053841590881348\n",
      "loss:2.3033299446105957\n",
      "loss:2.3046865463256836\n",
      "loss:2.3021984100341797\n",
      "loss:2.3030107021331787\n",
      "loss:2.302826404571533\n",
      "loss:2.305319309234619\n",
      "loss:2.3032772541046143\n",
      "loss:2.302579164505005\n",
      "loss:2.3035659790039062\n",
      "loss:2.302255392074585\n",
      "loss:2.301088333129883\n",
      "loss:2.303183078765869\n",
      "loss:2.3012492656707764\n",
      "loss:2.302947521209717\n",
      "loss:2.300877809524536\n",
      "loss:2.3024983406066895\n",
      "loss:2.3014793395996094\n",
      "loss:2.3045480251312256\n",
      "loss:2.304051160812378\n",
      "loss:2.30357027053833\n",
      "loss:2.3015782833099365\n",
      "loss:2.3022572994232178\n",
      "loss:2.303029775619507\n",
      "loss:2.303035020828247\n",
      "loss:2.3039958477020264\n",
      "loss:2.3019113540649414\n",
      "loss:2.3015642166137695\n",
      "loss:2.3014256954193115\n",
      "loss:2.302500009536743\n",
      "loss:2.3022871017456055\n",
      "loss:2.3041956424713135\n",
      "loss:2.29923939704895\n",
      "loss:2.3053524494171143\n",
      "loss:2.3036019802093506\n",
      "loss:2.3050029277801514\n",
      "loss:2.3052871227264404\n",
      "loss:2.3028786182403564\n",
      "loss:2.3037383556365967\n",
      "loss:2.3015286922454834\n",
      "loss:2.3044354915618896\n",
      "loss:2.3035154342651367\n",
      "loss:2.3014113903045654\n",
      "loss:2.3010385036468506\n",
      "loss:2.30330491065979\n",
      "loss:2.3032939434051514\n",
      "loss:2.301494598388672\n",
      "loss:2.305454969406128\n",
      "loss:2.3057074546813965\n",
      "loss:2.303433418273926\n",
      "loss:2.305004596710205\n",
      "loss:2.3033034801483154\n",
      "loss:2.3004112243652344\n",
      "loss:2.30102276802063\n",
      "loss:2.305206537246704\n",
      "loss:2.3019731044769287\n",
      "loss:2.302870750427246\n",
      "loss:2.304544687271118\n",
      "loss:2.303739309310913\n",
      "loss:2.301988124847412\n",
      "loss:2.299943685531616\n",
      "loss:2.302218198776245\n",
      "loss:2.3028833866119385\n",
      "loss:2.302116870880127\n",
      "loss:2.3045060634613037\n",
      "loss:2.3025829792022705\n",
      "loss:2.3028910160064697\n",
      "loss:2.301704168319702\n",
      "loss:2.3053078651428223\n",
      "loss:2.3014049530029297\n",
      "loss:2.303206443786621\n",
      "loss:2.3034608364105225\n",
      "loss:2.3048055171966553\n",
      "loss:2.30049467086792\n",
      "loss:2.303473949432373\n",
      "loss:2.300544500350952\n",
      "loss:2.3044421672821045\n",
      "loss:2.3052704334259033\n",
      "loss:2.303298234939575\n",
      "loss:2.302388906478882\n",
      "loss:2.3007307052612305\n",
      "loss:2.302617073059082\n",
      "loss:2.304457426071167\n",
      "loss:2.302671194076538\n",
      "loss:2.303830862045288\n",
      "loss:2.3028180599212646\n",
      "loss:2.3024253845214844\n",
      "loss:2.303945541381836\n",
      "loss:2.3040647506713867\n",
      "loss:2.300311326980591\n",
      "loss:2.303813934326172\n",
      "loss:2.3032732009887695\n",
      "loss:2.306042432785034\n",
      "loss:2.303858518600464\n",
      "loss:2.3028993606567383\n",
      "loss:2.3045125007629395\n",
      "loss:2.303774833679199\n",
      "loss:2.303105592727661\n",
      "loss:2.3039045333862305\n",
      "loss:2.302495002746582\n",
      "loss:2.3030426502227783\n",
      "loss:2.3059611320495605\n",
      "loss:2.3048269748687744\n",
      "loss:2.3031044006347656\n",
      "loss:2.301297664642334\n",
      "loss:2.3008947372436523\n",
      "loss:2.30350923538208\n",
      "loss:2.3031842708587646\n",
      "loss:2.3030261993408203\n",
      "loss:2.3023746013641357\n",
      "loss:2.3055598735809326\n",
      "loss:2.303635597229004\n",
      "loss:2.303375005722046\n",
      "loss:2.303328037261963\n",
      "loss:2.3026697635650635\n",
      "loss:2.302684783935547\n",
      "loss:2.3024961948394775\n",
      "loss:2.3018059730529785\n",
      "loss:2.3027231693267822\n",
      "loss:2.304912805557251\n",
      "loss:2.3014330863952637\n",
      "loss:2.303910493850708\n",
      "loss:2.3023619651794434\n",
      "loss:2.303847551345825\n",
      "loss:2.3042259216308594\n",
      "loss:2.3041536808013916\n",
      "loss:2.303407669067383\n",
      "loss:2.3018667697906494\n",
      "loss:2.302957773208618\n",
      "loss:2.3061273097991943\n",
      "loss:2.303455352783203\n",
      "loss:2.30338716506958\n",
      "loss:2.302414655685425\n",
      "loss:2.3013503551483154\n",
      "loss:2.304361343383789\n",
      "loss:2.3040733337402344\n",
      "loss:2.3026955127716064\n",
      "loss:2.3033905029296875\n",
      "loss:2.3041205406188965\n",
      "loss:2.305417776107788\n",
      "loss:2.302595376968384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.3005597591400146\n",
      "loss:2.306140661239624\n",
      "loss:2.301236867904663\n",
      "loss:2.303288698196411\n",
      "loss:2.3035645484924316\n",
      "loss:2.3030619621276855\n",
      "loss:2.300976514816284\n",
      "loss:2.3038835525512695\n",
      "loss:2.3009986877441406\n",
      "loss:2.2993991374969482\n",
      "loss:2.302459716796875\n",
      "loss:2.3021011352539062\n",
      "loss:2.3048675060272217\n",
      "loss:2.3023841381073\n",
      "loss:2.3009865283966064\n",
      "loss:2.303483724594116\n",
      "loss:2.304835081100464\n",
      "loss:2.3037171363830566\n",
      "loss:2.3047544956207275\n",
      "loss:2.3009605407714844\n",
      "loss:2.3049561977386475\n",
      "loss:2.3047823905944824\n",
      "loss:2.3036422729492188\n",
      "loss:2.3034873008728027\n",
      "loss:2.303422689437866\n",
      "loss:2.3034863471984863\n",
      "loss:2.30195689201355\n",
      "loss:2.304194450378418\n",
      "loss:2.302464008331299\n",
      "loss:2.3032121658325195\n",
      "loss:2.3029026985168457\n",
      "loss:2.305100679397583\n",
      "loss:2.3041346073150635\n",
      "loss:2.3038811683654785\n",
      "loss:2.3014450073242188\n",
      "loss:2.302356004714966\n",
      "loss:2.3032772541046143\n",
      "loss:2.301088809967041\n",
      "loss:2.304750442504883\n",
      "loss:2.3061392307281494\n",
      "loss:2.304527997970581\n",
      "loss:2.300252914428711\n",
      "loss:2.3029942512512207\n",
      "loss:2.3037261962890625\n",
      "loss:2.3022639751434326\n",
      "loss:2.304286241531372\n",
      "loss:2.3030951023101807\n",
      "loss:2.303363084793091\n",
      "loss:2.304372549057007\n",
      "loss:2.3019895553588867\n",
      "loss:2.3040568828582764\n",
      "loss:2.30245041847229\n",
      "loss:2.303889751434326\n",
      "loss:2.304425001144409\n",
      "loss:2.3036632537841797\n",
      "loss:2.30155611038208\n",
      "loss:2.303051710128784\n",
      "loss:2.3026297092437744\n",
      "loss:2.3054418563842773\n",
      "loss:2.3018529415130615\n",
      "loss:2.3025875091552734\n",
      "loss:2.30269718170166\n",
      "loss:2.3040108680725098\n",
      "loss:2.3021953105926514\n",
      "loss:2.302276849746704\n",
      "loss:2.3037302494049072\n",
      "loss:2.3034887313842773\n",
      "loss:2.303269147872925\n",
      "loss:2.300130844116211\n",
      "loss:2.3031582832336426\n",
      "loss:2.3034613132476807\n",
      "loss:2.305015802383423\n",
      "loss:2.3033437728881836\n",
      "loss:2.302574634552002\n",
      "loss:2.30389404296875\n",
      "loss:2.303039789199829\n",
      "loss:2.3036632537841797\n",
      "loss:2.303494691848755\n",
      "loss:2.30185866355896\n",
      "loss:2.3042492866516113\n",
      "loss:2.30476975440979\n",
      "loss:2.3022334575653076\n",
      "loss:2.302961826324463\n",
      "loss:2.3048174381256104\n",
      "loss:2.302842855453491\n",
      "loss:2.302739143371582\n",
      "loss:2.301654100418091\n",
      "loss:2.3024041652679443\n",
      "loss:2.3030929565429688\n",
      "loss:2.3021042346954346\n",
      "loss:2.3022541999816895\n",
      "loss:2.30364990234375\n",
      "loss:2.3038392066955566\n",
      "loss:2.3028132915496826\n",
      "loss:2.3031387329101562\n",
      "loss:2.302250623703003\n",
      "loss:2.3042335510253906\n",
      "loss:2.301699161529541\n",
      "loss:2.3040990829467773\n",
      "loss:2.3021798133850098\n",
      "loss:2.303040027618408\n",
      "loss:2.302643299102783\n",
      "loss:2.305493116378784\n",
      "loss:2.305677652359009\n",
      "loss:2.30139422416687\n",
      "loss:2.302508592605591\n",
      "loss:2.303258180618286\n",
      "loss:2.300316333770752\n",
      "loss:2.3039610385894775\n",
      "loss:2.3040804862976074\n",
      "loss:2.3038032054901123\n",
      "loss:2.3007330894470215\n",
      "loss:2.305521249771118\n",
      "loss:2.303297281265259\n",
      "loss:2.302427053451538\n",
      "loss:2.302067518234253\n",
      "loss:2.303961753845215\n",
      "loss:2.3037705421447754\n",
      "loss:2.3043909072875977\n",
      "loss:2.301999568939209\n",
      "loss:2.3033034801483154\n",
      "loss:2.3027889728546143\n",
      "loss:2.3012657165527344\n",
      "loss:2.304287910461426\n",
      "loss:2.3035941123962402\n",
      "loss:2.30190110206604\n",
      "loss:2.303663492202759\n",
      "loss:2.3022940158843994\n",
      "loss:2.3035759925842285\n",
      "loss:2.301858901977539\n",
      "loss:2.3018412590026855\n",
      "loss:2.304068088531494\n",
      "loss:2.3035888671875\n",
      "loss:2.305882692337036\n",
      "loss:2.3023569583892822\n",
      "loss:2.3019723892211914\n",
      "loss:2.302125930786133\n",
      "loss:2.3021199703216553\n",
      "loss:2.303117513656616\n",
      "loss:2.30192494392395\n",
      "loss:2.3036110401153564\n",
      "loss:2.3047702312469482\n",
      "loss:2.30521821975708\n",
      "loss:2.301929473876953\n",
      "loss:2.303893804550171\n",
      "loss:2.3050506114959717\n",
      "loss:2.3055434226989746\n",
      "loss:2.300842046737671\n",
      "loss:2.3003413677215576\n",
      "loss:2.30541729927063\n",
      "loss:2.3024306297302246\n",
      "loss:2.303581476211548\n",
      "loss:2.3048129081726074\n",
      "loss:2.3022773265838623\n",
      "loss:2.3049986362457275\n",
      "loss:2.3029239177703857\n",
      "loss:2.304300308227539\n",
      "loss:2.3054275512695312\n",
      "loss:2.302703857421875\n",
      "loss:2.3038763999938965\n",
      "loss:2.302255392074585\n",
      "loss:2.3027546405792236\n",
      "loss:2.3035693168640137\n",
      "loss:2.303628444671631\n",
      "loss:2.304929733276367\n",
      "loss:2.3034679889678955\n",
      "loss:2.304387092590332\n",
      "loss:2.302861213684082\n",
      "loss:2.303420066833496\n",
      "loss:2.3053245544433594\n",
      "loss:2.302645683288574\n",
      "loss:2.302334785461426\n",
      "loss:2.3057241439819336\n",
      "loss:2.3038854598999023\n",
      "loss:2.3031415939331055\n",
      "loss:2.302780866622925\n",
      "loss:2.3029463291168213\n",
      "loss:2.304304599761963\n",
      "loss:2.30230712890625\n",
      "loss:2.3068833351135254\n",
      "loss:2.302964210510254\n",
      "loss:2.3036410808563232\n",
      "loss:2.3039350509643555\n",
      "loss:2.3024919033050537\n",
      "loss:2.3022007942199707\n",
      "loss:2.3022868633270264\n",
      "loss:2.3014369010925293\n",
      "loss:2.301440477371216\n",
      "loss:2.3024704456329346\n",
      "loss:2.3071186542510986\n",
      "loss:2.3017807006835938\n",
      "loss:2.3026630878448486\n",
      "loss:2.3050410747528076\n",
      "loss:2.3034451007843018\n",
      "loss:2.303598403930664\n",
      "loss:2.301809549331665\n",
      "loss:2.3033559322357178\n",
      "loss:2.3015854358673096\n",
      "loss:2.3030052185058594\n",
      "loss:2.3014369010925293\n",
      "loss:2.307246208190918\n",
      "loss:2.3015506267547607\n",
      "loss:2.302541732788086\n",
      "loss:2.3025248050689697\n",
      "loss:2.3032917976379395\n",
      "loss:2.303211212158203\n",
      "loss:2.301774263381958\n",
      "loss:2.302107334136963\n",
      "loss:2.3033478260040283\n",
      "loss:2.3042969703674316\n",
      "loss:2.3009049892425537\n",
      "loss:2.3037269115448\n",
      "loss:2.302332639694214\n",
      "loss:2.304523229598999\n",
      "loss:2.3033740520477295\n",
      "loss:2.3048951625823975\n",
      "loss:2.3046891689300537\n",
      "loss:2.3019542694091797\n",
      "loss:2.3069424629211426\n",
      "loss:2.301668643951416\n",
      "loss:2.302528142929077\n",
      "loss:2.3001668453216553\n",
      "loss:2.3024463653564453\n",
      "loss:2.303558588027954\n",
      "loss:2.3014180660247803\n",
      "loss:2.3002257347106934\n",
      "loss:2.30400013923645\n",
      "loss:2.303417921066284\n",
      "loss:2.3010621070861816\n",
      "loss:2.303507089614868\n",
      "loss:2.302696704864502\n",
      "loss:2.2995786666870117\n",
      "loss:2.304502487182617\n",
      "loss:2.302873373031616\n",
      "loss:2.3029911518096924\n",
      "loss:2.3018221855163574\n",
      "loss:2.301666498184204\n",
      "loss:2.3045616149902344\n",
      "loss:2.300870895385742\n",
      "loss:2.3023459911346436\n",
      "loss:2.3027842044830322\n",
      "loss:2.306718349456787\n",
      "loss:2.3030617237091064\n",
      "loss:2.3030776977539062\n",
      "loss:2.303011417388916\n",
      "loss:2.3062357902526855\n",
      "loss:2.3036246299743652\n",
      "loss:2.303222417831421\n",
      "loss:2.3040077686309814\n",
      "loss:2.3053054809570312\n",
      "loss:2.3034517765045166\n",
      "loss:2.304989814758301\n",
      "loss:2.3044273853302\n",
      "loss:2.3028371334075928\n",
      "loss:2.303596258163452\n",
      "loss:2.305347442626953\n",
      "loss:2.3023152351379395\n",
      "loss:2.302882671356201\n",
      "loss:2.303239583969116\n",
      "loss:2.3016066551208496\n",
      "loss:2.3038413524627686\n",
      "loss:2.3039679527282715\n",
      "loss:2.302328586578369\n",
      "loss:2.3013176918029785\n",
      "loss:2.304034948348999\n",
      "loss:2.3023769855499268\n",
      "loss:2.301652193069458\n",
      "loss:2.3044381141662598\n",
      "loss:2.3040831089019775\n",
      "loss:2.30163311958313\n",
      "loss:2.3042235374450684\n",
      "loss:2.302239179611206\n",
      "loss:2.3021891117095947\n",
      "loss:2.305541515350342\n",
      "loss:2.303237199783325\n",
      "loss:2.305495262145996\n",
      "loss:2.3016204833984375\n",
      "loss:2.304166793823242\n",
      "loss:2.3027291297912598\n",
      "loss:2.3021605014801025\n",
      "loss:2.3023080825805664\n",
      "loss:2.301271438598633\n",
      "loss:2.298652172088623\n",
      "loss:2.305307149887085\n",
      "loss:2.303105354309082\n",
      "loss:2.302159070968628\n",
      "loss:2.303205728530884\n",
      "loss:2.302476406097412\n",
      "loss:2.3001880645751953\n",
      "loss:2.3010334968566895\n",
      "loss:2.3006670475006104\n",
      "loss:2.3045482635498047\n",
      "loss:2.305692195892334\n",
      "loss:2.3050742149353027\n",
      "loss:2.3054656982421875\n",
      "loss:2.300724506378174\n",
      "loss:2.302696466445923\n",
      "loss:2.3007805347442627\n",
      "loss:2.3041794300079346\n",
      "loss:2.3041629791259766\n",
      "loss:2.30202579498291\n",
      "loss:2.3030574321746826\n",
      "loss:2.3012959957122803\n",
      "loss:2.305103302001953\n",
      "loss:2.304529905319214\n",
      "loss:2.3030967712402344\n",
      "loss:2.3031232357025146\n",
      "loss:2.3040473461151123\n",
      "loss:2.3037867546081543\n",
      "loss:2.304734468460083\n",
      "loss:2.3012547492980957\n",
      "loss:2.301525354385376\n",
      "loss:2.3043630123138428\n",
      "loss:2.304051637649536\n",
      "loss:2.302608013153076\n",
      "loss:2.3009192943573\n",
      "loss:2.303506374359131\n",
      "loss:2.305992841720581\n",
      "loss:2.3024892807006836\n",
      "loss:2.3062918186187744\n",
      "loss:2.301670789718628\n",
      "loss:2.3036890029907227\n",
      "loss:2.3031840324401855\n",
      "loss:2.302051544189453\n",
      "loss:2.3048508167266846\n",
      "loss:2.301621437072754\n",
      "loss:2.3031930923461914\n",
      "loss:2.3072941303253174\n",
      "loss:2.3046722412109375\n",
      "loss:2.3037171363830566\n",
      "loss:2.302643060684204\n",
      "loss:2.303952217102051\n",
      "loss:2.3038926124572754\n",
      "loss:2.30307936668396\n",
      "loss:2.30265212059021\n",
      "loss:2.3015570640563965\n",
      "loss:2.3001365661621094\n",
      "loss:2.301994562149048\n",
      "loss:2.2998576164245605\n",
      "loss:2.3043100833892822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-751d5a45c6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss:{loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    for i ,(input_,label) in enumerate(trainloader,0):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(net(input_),label)\n",
    "        loss.backward()\n",
    "        optimizer\n",
    "        print(f\"loss:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

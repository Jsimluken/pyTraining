{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet import EfficientNetB0\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from data_loader import DataLoader\n",
    "from adam_lr_mult import Adam_lr_mult\n",
    "\n",
    "def prepare_new_model(input_shape, class_count):\n",
    "    # 学習済みモデルの取り出し\n",
    "    feature_extractor = EfficientNetB0(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "    # 犬猫分類器を引っ付ける\n",
    "    x = feature_extractor.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Dense(class_count, activation='sigmoid')(x)\n",
    "    # 新たなモデルの定義\n",
    "    model = Model(inputs=feature_extractor.input, outputs=x)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_adam_for_fine_tuning(lr, decay, multiplier, model):\n",
    "    lr_multiplier = {}\n",
    "    # 自分が引っ付けたレイヤーの学習係数は1、学習済みの部分は小さな値を設定する\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:\n",
    "            lr_multiplier[layer.name] = 1.0\n",
    "        else:\n",
    "            lr_multiplier[layer.name] = multiplier\n",
    "    return Adam_lr_mult(lr=lr, decay=decay, multipliers=lr_multiplier)\n",
    "\n",
    "def train(epochs, batch_size, input_shape, class_count):\n",
    "    # 学習用画像データローダー\n",
    "    train_data_loader = DataLoader('train', batch_size, input_shape, do_augmentation=True)\n",
    "    train_generator = train_data_loader.get_data_loader()\n",
    "    # 検証用画像データローダー\n",
    "    val_data_loader = DataLoader('val', batch_size, input_shape, do_augmentation=False)\n",
    "    val_generator = val_data_loader.get_data_loader()\n",
    "    # モデルの生成\n",
    "    model = prepare_new_model(input_shape, class_count)\n",
    "    # ファインチューニング用Adamオプティマイザ\n",
    "    optimizer = get_adam_for_fine_tuning(lr=1e-3, decay=1e-5, multiplier=0.01, model=model)\n",
    "    # コンパイルして\n",
    "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit_generatorするだけ\n",
    "    h = model.fit_generator(train_generator, train_data_loader.iterations, epochs,\n",
    "                                  validation_data=val_generator, validation_steps=val_data_loader.iterations)\n",
    "    # 学習データ、検証データのロスとAccをファイルに出力\n",
    "    with open('loss.csv', 'a') as f:\n",
    "        for loss_t, acc_t, loss_v, acc_v in zip(h.history['loss'], h.history['acc'], h.history['val_loss'], h.history['val_acc']):\n",
    "            f.write(str(loss_t) + ',' + str(acc_t) + ',' + str(loss_v) + ',' + str(acc_v) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    epochs = 5\n",
    "    batch_size = 16\n",
    "    input_shape = (224, 224, 3)\n",
    "    class_count = 2\n",
    "    train(epochs, batch_size, input_shape, class_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
